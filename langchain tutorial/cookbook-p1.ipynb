{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "029e3ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chat\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.7, openai_api_key=\"...openai_api_key...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2560d065",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "119f6d8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='You could try a Caprese salad with fresh tomatoes, mozzarella, and basil.', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\n",
    "    [\n",
    "SystemMessage(content=\"You are a nice AI bot that helps a user figure out what to eat in one short sentence\"),\n",
    "HumanMessage(content=\"I like tomatoes, what should I eat?\")\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "318d124a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Nice is a beautiful city located on the French Riviera, and there are many things to do besides enjoying the beaches. Here are some suggestions:\\n\\n1. Walk along the Promenade des Anglais, a famous beachside promenade that stretches for miles.\\n2. Explore the winding streets of Old Town, where you can find charming shops, cafes, and restaurants.\\n3. Visit the Mus√©e Matisse, which houses a large collection of works by the famous French artist.\\n4. Take a day trip to Monaco, which is just a short train ride away.\\n5. Enjoy the local cuisine, which features fresh seafood and delicious regional dishes.\\n6. Visit the Russian Orthodox Cathedral, a stunning architectural masterpiece.\\n7. Take a sunset cruise along the coast to see the city from a different perspective.\\n8. Go hiking in the nearby hills for stunning views of the city and the sea.', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat([\n",
    "    SystemMessage(content=\"You are a nice AI bot that helps a user figure out where to travel in one short\"),\n",
    "    HumanMessage(content=\"I like the beaches where should I go?\"),\n",
    "    AIMessage(content=\"You should go to Nice,France\"),\n",
    "    HumanMessage(content=\"What else should I do when I am there\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2ab38ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#document\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b0f76ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"This is my document. It is full of text that I've gathered from other places\", metadata={'my_document_id': 234234, 'my_doument_source': 'The Langchain Papers', 'my_document_create_time': 1680013019})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Document(page_content=\"This is my document. It is full of text that I've gathered from other places\",metadata={'my_document_id':234234,\n",
    "                                                                                                              'my_doument_source':\"The Langchain Papers\",\n",
    "                                                                                                              'my_document_create_time':1680013019})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c20e1a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#language model - text in text out\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"text-ada-001\",openai_api_key=\"...openai_api_key...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d1ce152",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nSaturday.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"What day comes after Friday?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3df0cb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chat model - a model that takes a series of messages and returns message output\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "chat = ChatOpenAI(temperature=1,openai_api_key=\"...openai_api_key...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "139f5768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Oh, just flap your arms really hard and maybe you'll fly there! Ha ha ha ha!\", additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat([\n",
    "    SystemMessage(content=\"You are an unhelpful AI bot that makes a joke at whatever the user says\"),\n",
    "     #SystemMessage(content=\"You are the most helpful indian AI bot that does its best whatever user says\"),\n",
    "    HumanMessage(content=\"I would like to go to New York, how should I do this?\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32e5380c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Embedding Model - Change your text into a vector(a series of numbers that hold the semantic 'meaning of your text'), Mainly used when comparing two pieces of text together.\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=\"...openai_api_key...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6965b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hi! It's time for the beach\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "92bef4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your embeddings is length 1536\n",
      "Here's a sample: [-0.00015641732898075134, -0.003165106289088726, -0.000814014405477792, -0.019451458007097244, -0.01518280804157257]...\n"
     ]
    }
   ],
   "source": [
    "text_embedding = embeddings.embed_query(text)\n",
    "print(f\"Your embeddings is length {len(text_embedding)}\")\n",
    "print(f\"Here's a sample: {text_embedding[:5]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc8dc6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nThe statement is incorrect because tomorrow is Tuesday, not Wednesday.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Prompt - Text generally used as instructions to your model\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\",openai_api_key=\"...openai_api_key...\")\n",
    "\n",
    "# I like to use three double quotation marks for my prompts because it's easier to read\n",
    "prompt = \"\"\"\n",
    "Today is Monday, tomorrow is Wednesday.\n",
    "\n",
    "What is wrong with that statement?\n",
    "\"\"\"\n",
    "\n",
    "llm(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2749827f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Prompt: \n",
      "I really want to travel to Rome. What should I do there?\n",
      "\n",
      "Respond in one short sentence\n",
      "\n",
      "\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Output: \n",
      "Explore the Colosseum, the Roman Forum, and the Pantheon to take in the ancient history of the city.\n"
     ]
    }
   ],
   "source": [
    "# Prompt Template - An object that helps create prompts based on a combination of user input, other non- static information and a fixed template string.\n",
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", openai_api_key=\"...openai_api_key...\")\n",
    "\n",
    "# Notice \"location\" below, that is a placeholder for another value later\n",
    "template = \"\"\"\n",
    "I really want to travel to {location}. What should I do there?\n",
    "\n",
    "Respond in one short sentence\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "input_variables = [\"location\"],\n",
    "template= template,\n",
    ")\n",
    "\n",
    "final_prompt = prompt.format(location=\"Rome\")\n",
    "\n",
    "print(f\"Final Prompt: {final_prompt}\")\n",
    "print(\"----------------\")\n",
    "print(f\"LLM Output: {llm(final_prompt)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b034ab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Selectors \n",
    "# An easy way to select from a series of examples that allow you to dynamic place in-context information into your prompt. Often used when your task is nuanced or you have a large list of exmaples.\n",
    "\n",
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\",openai_api_key=\"...openai_api_key...\")\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "input_variables = [\"input\",\"output\"],\n",
    "template=\"Example Input: {input}\\nExample Output: {output}\",    \n",
    ")\n",
    "\n",
    "# Example of location that nouns are found\n",
    "examples = [\n",
    "    {\"input\": \"pirate\", \"output\": \"ship\"},\n",
    "    {\"input\": \"pilot\", \"output\": \"plane\"},\n",
    "    {\"input\": \"driver\", \"output\" :\"car\"},\n",
    "    {\"input\": \"tree\", \"output\" :\"sky\"},\n",
    "    {\"input\": \"bird\", \"output\": \"nest\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d484ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SemanticsSimilarityExampleSelector will select examples that are similar to your input by semantics\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    #This is the list of examples available to select from.\n",
    "    examples,\n",
    "    \n",
    "    # This is the embedding class used to produce embeddings which are used to measure semantic\n",
    "    OpenAIEmbeddings(openai_api_key=\"...openai_api_key...\"),\n",
    "    \n",
    "    # This is the VectorStore class that is used to store the embeddings and do a similarity semantic\n",
    "    FAISS,\n",
    "    \n",
    "    # This is the number of examples to produce.\n",
    "    k=2\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "41b3ad00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.7.4-cp310-cp310-win_amd64.whl (10.8 MB)\n",
      "     ---------------------------------------- 10.8/10.8 MB 3.9 MB/s eta 0:00:00\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.7.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "675332a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SemanticsSimilarityExampleSelector will select examples that are similar to your input by semantics\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    #This is the list of examples available to select from.\n",
    "    examples,\n",
    "    \n",
    "    # This is the embedding class used to produce embeddings which are used to measure semantic\n",
    "    OpenAIEmbeddings(openai_api_key=\"...openai_api_key...\"),\n",
    "    \n",
    "    # This is the VectorStore class that is used to store the embeddings and do a similarity semantic\n",
    "    FAISS,\n",
    "    \n",
    "    # This is the number of examples to produce.\n",
    "    k=2\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0c5ad4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_prompt = FewShotPromptTemplate(\n",
    "# The object that will help select examples\n",
    "    example_selector= example_selector,\n",
    " \n",
    "    # Your prompt\n",
    "    example_prompt=example_prompt,\n",
    "    \n",
    "    #Customizations that will be added to the top and bottom of your prompt\n",
    "    prefix= \"Give the location an item is usually found in\",\n",
    "    suffix=\"Input: {noun}\\nOutput\",\n",
    "    \n",
    "    #What inputs your prompt will receive\n",
    "    input_variables=[\"noun\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "97813c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give the location an item is usually found in\n",
      "\n",
      "Example Input: driver\n",
      "Example Output: car\n",
      "\n",
      "Example Input: pilot\n",
      "Example Output: plane\n",
      "\n",
      "Input: student\n",
      "Output\n"
     ]
    }
   ],
   "source": [
    "#select a noun!\n",
    "my_noun = \"student\"\n",
    "print(similar_prompt.format(noun=my_noun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ca0a5ec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "': garden'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(similar_prompt.format(noun=my_noun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1d3c98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Parser - A helpful way to format the output of a model. Usually used for structured output.\n",
    "# Two big concepts: 1. Format Instructions - An autogenerated prompts that tells the LLM how to format it's response based off your desired result.\n",
    "# 2. Parser - A method which will extract your model's text output into a desired structure(usually json)\n",
    "\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7bc5022",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model_name=\"text-davinci-003\", openai_api_key=\"...openai_api_key...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8de6eee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How you would like your response structured. This is basically a fancy prompt template\n",
    "response_schemas= [\n",
    "    ResponseSchema(name=\"bad_string\", description=\" This a poorly formatted user input string\"),\n",
    "    ResponseSchema(name=\"good_string\", description=\"This is your response, a reformatted response\")\n",
    "]\n",
    "\n",
    "# How you would like to parse your output\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1279777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"\\`\\`\\`json\" and \"\\`\\`\\`\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"bad_string\": string  //  This a poorly formatted user input string\n",
      "\t\"good_string\": string  // This is your response, a reformatted response\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# see the prompt template you created for formatting\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "473408e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You will be given a poorly formatted string from a user.\n",
      "Reformat it and make sure all the words are spelled correctly\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"\\`\\`\\`json\" and \"\\`\\`\\`\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"bad_string\": string  //  This a poorly formatted user input string\n",
      "\t\"good_string\": string  // This is your response, a reformatted response\n",
      "}\n",
      "```\n",
      "\n",
      "% USER_INPUT\n",
      "welcom to califonya!\n",
      "\n",
      "YOUR RESPONSE:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "You will be given a poorly formatted string from a user.\n",
    "Reformat it and make sure all the words are spelled correctly\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "% USER_INPUT\n",
    "{user_input}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables = [\"user_input\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    "    template=template\n",
    ")\n",
    "\n",
    "promptValue = prompt.format(user_input=\"welcom to califonya!\")\n",
    "\n",
    "print(promptValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53ffedc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8d66f88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n{\\n\\t\"bad_string\": \"welcom to califonya!\",\\n\\t\"good_string\": \"Welcome to California!\"\\n}\\n```'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_output = llm(promptValue)\n",
    "llm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f51abbb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bad_string': 'welcom to califonya!', 'good_string': 'Welcome to California!'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.parse(llm_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2035ed2b",
   "metadata": {},
   "source": [
    "## Indexes - Structuring documents to LLMs can work with them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743c4088",
   "metadata": {},
   "source": [
    "### Document Loaders\n",
    "Easy ways to import data from other sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e20db42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import HNLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f60635bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = HNLoader(\"https://news.ycombinator.com/item?id=34422627\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd49b184",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d54ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Found {len(data)} comments\")\n",
    "print(f\"Here's a sample:\\n\\n{''.join([x.page_content[:150] for x in data[:2]])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdd567f",
   "metadata": {},
   "source": [
    "## Text Splitters\n",
    "Often times your document is too long(like a book) for your LLM. You need to split it up into chunks. Text splitters help with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c336cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f5c0e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 1 document\n"
     ]
    }
   ],
   "source": [
    "# This is a long document we can split up.\n",
    "with open('data/PaulGrahamEssays/worked.txt') as f:\n",
    "    pg_work = f.read()\n",
    "    \n",
    "print(f\"You have {len([pg_work])} document\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2440c740",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 20,\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents({pg_work})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5a784c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 78 documents\n"
     ]
    }
   ],
   "source": [
    "print(f\"You have {len(texts)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fd05ce8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview:\n",
      "February 2021Before college the two main things I worked on, outside of school,\n",
      "were writing and programming. I didn't write essays. I wrote what\n",
      "beginning writers were supposed to write then, and probably still\n",
      "are: short stories. My stories were awful. They had hardly any plot,\n",
      "just characters with strong feelings, which I imagined made them\n",
      "deep.The first programs I tried writing were on the IBM 1401 that our\n",
      "school district used for what was then called \"data processing.\"\n",
      "This was in 9th grade, so I was 13 or 14. The school district's\n",
      "1401 happened to be in the basement of our junior high school, and\n",
      "my friend Rich Draves and I got permission to use it. It was like\n",
      "a mini Bond villain's lair down there, with all these alien-looking\n",
      "machines √Ç‚Äî CPU, disk drives, printer, card reader √Ç‚Äî sitting up\n",
      "on a raised floor under bright fluorescent lights.The language we used was an early version of Fortran. You had to\n",
      "type programs on punch cards, then stack them in the card reader \n",
      "\n",
      "and press a button to load the program into memory and run it. The\n",
      "result would ordinarily be to print something on the spectacularly\n",
      "loud printer.I was puzzled by the 1401. I couldn't figure out what to do with\n",
      "it. And in retrospect there's not much I could have done with it.\n",
      "The only form of input to programs was data stored on punched cards,\n",
      "and I didn't have any data stored on punched cards. The only other\n",
      "option was to do things that didn't rely on any input, like calculate\n",
      "approximations of pi, but I didn't know enough math to do anything\n",
      "interesting of that type. So I'm not surprised I can't remember any\n",
      "programs I wrote, because they can't have done much. My clearest\n",
      "memory is of the moment I learned it was possible for programs not\n",
      "to terminate, when one of mine didn't. On a machine without\n",
      "time-sharing, this was a social as well as a technical error, as\n",
      "the data center manager's expression made clear.With microcomputers, everything changed. Now you could have a\n"
     ]
    }
   ],
   "source": [
    "print(\"Preview:\")\n",
    "print(texts[0].page_content, \"\\n\")\n",
    "print(texts[1].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4827ba7b",
   "metadata": {},
   "source": [
    "## Retrievers\n",
    "Easy way to combine document with language models.\n",
    "There are many different types of retrievers, the most widely supported is the vectorStoreRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cf0335c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "loader = TextLoader('data/PaulGrahamEssays/worked.txt')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b3022d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get your splitter ready\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=50)\n",
    "\n",
    "# Split your docs into texts\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# Get embedding engine ready\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=\"...openai_api_key...\")\n",
    "\n",
    "# Embedd your texts\n",
    "db = FAISS.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ab8aeab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init your retriever. Asking for just 1 document back\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a876fe63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(vectorstore=<langchain.vectorstores.faiss.FAISS object at 0x000002F83763DA20>, search_type='similarity', search_kwargs={})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bc54a12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')).\n"
     ]
    }
   ],
   "source": [
    "docs = retriever.get_relevant_documents(\"What types of things did the author want to build?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "453463a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standards; what was the point? No one else wanted one either, so\n",
      "off they went. That was what happened to systems work.I wanted not just to build things, but to build things that would\n",
      "last.In this di\n",
      "\n",
      "infrastructure, and the two undergrads worked on the first two\n",
      "services (images and phone calls). But about halfway through the\n",
      "summer I realized I really didn't want to run a company √Ç‚Äî especially\n",
      "no\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n\".join([x.page_content[:200] for x in docs[:2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9b1919",
   "metadata": {},
   "source": [
    "## VectorStores\n",
    "Database to store vectors, Most popular ones are Pine & Weaviate\n",
    "Conceptually, think of them as tables q/ a column for embeddings(vectors) and a column for metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1a893f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "loader = TextLoader(\"data/PaulGrahamEssays/worked.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Get your splitter ready\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "\n",
    "# Split your docs into texts\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# Get embeddings engine ready\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=\"...openai_api_key...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55e1e9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
